// -*- coding: utf-8 -*-
:encoding: UTF-8

New Distribution Whitepaper
==========================
:Author: Lars Hagström, Anders Widén and Joel Ottosson
28 Mar 2013

== Preface
The purpose of this document is to describe the new distribution mechanism of Safir SDK Core and the related changes. It contains information on the benefits, the requirements and the design.

== Benefits
Robust communication::
  Exclusion of misbehaving nodes;;
    New implementation will have an algorithm that will automatically exclude a node that does not behave properly.
  Slow node will not slow down all nodes;;
    Current implementation means that one node that is "slow", either temporarily or always, will slow down distribution to all other nodes.
  Support for other topologies;;
    A number of issues with the current implementation will be addressed:
    - Is really designed and optimized for LAN with TTL=1. It has been made to work with higher TTL. 
    - Has no support for networks where all nodes cannot see all other nodes via UDP multicast.
    - Does not detect situations where firewalls make communication become "one-way".
Removal of general "join" will simplify design::
  The attempt to add general "join" functionality added a lot of complexity that is very unpredictable and hard to explain/document. Removing the general join will simplify the software.

Addition of specific "join" cases will allow more uses::
  Instead of a general join we want to add two kinds of specific join, which will allow nodes to be reconnected if  a network connection is broken. These kinds of nodes will not support having redundant entities:
  Disconnected node will keep all entities owned on other nodes as "read only";;
    An operator console with this setting will still be able to display the "last known" state of all entities. No modification will be allowed and no requests will be served.
  Disconnected node will become empty;;
    An operator console with this setting will still become "empty", i.e. all non-local entities will be deleted (this is the current behavior). When the node is reconnected it will receive all remote entities again. 

Less static configuration::
  We want to provide a "dhcp"-like functionality for configuring nodes, so that instead of configuring the node identities and other details that should be internal you specify a node type, e.g. "operator console" or "Database server", and from that we internally work out what node id a node should have. This would make adding new nodes a lot easier.

Easier to display system status::
  We want to provide a unified picture of the system topology, both for use by the system itself to make sure the system remains robust, but this information can also be displayed to allow users and developers to understand what the system status is.

Simpler to start system::
  Less needs for start scripts. All parts of Safir SDK Core and the rest of the system gets started by the same mechanism.

== Product Evaluation
Preferrably some 3rd party products or protocols should be used in the development of the new distribution mechanism, to simplify and speed up the development process, and to reduce maintenance costs.

Some requirements:

Platform independence of 3rd party dependencies::
  Chosen technologies shall run on Win32 and Linux.
License::
  Chosen technologies shall have a GPL compatible license, since Safir SDK Core license is GPLv3. See http://www.gnu.org/licenses/license-list.html for the GNU project list of compatible licenses.
Real-Time Behaviour::
  Chosen technologies shall not prevent the Safir SDK Core from having bounded latency.
Reliability::
  Chosen technologies shall not prevent the Safir SDK Core from providing reliability guarantees.
Complexity::
  Chosen technologies shall not add unwarranted complexity to configuration or use.

=== Data format / Serialization
Using an open or standard format for the data packets would remove the need of "rolling our own", and probably also easier to use third party tools such as Wireshark for debugging.

We want a binary format since the blobs are already binary, so XML- or JSON-based formats have not been investigated.

In some distant future we might also want to change the blob format to use an open format, but in the short term we will just wrap them in whatever format we choose.

We only need support for C\+\+, since all the data transport and blob packing/unpacking code is C++ code.

Protobuf - http://code.google.com/p/protobuf/::
  * Google developed data serialization format. 
  * Used by almost all Google internal RPC and file formats. 
  * Well documented and well supported (by Google).
  * There are also multiple implementations, e.g. SAX-like deserialization libraries are available.
  * Stable and widely used.
  * BSD 3-Clause License (GPL compatible)
Apache Thrift - http://thrift.apache.org/::
  * Interfaces to more languages than Protobuf.
  * Slightly larger serialized results compared to Protobuf.
  * Maintenance seems less "professional" than Protobuf.
  * Apache License 2.0 (GPLv3 compatible)

==== Conclusion
*Protobuf* seems to be the sensible choice. Wider usage, better documentation and better maintenance/development.

=== Distribution mechanisms
There are a lot of possible products and protocols available. They all have different characteristics and different levels of guarantees. Here is a list of some of the main ones that we have evaluated along with some of our thoughts on them. See <<discarded_solutions, Discarded Solutions>> for further details into the protocols and products that we have investigated more thoroughly.

DDS - http://en.wikipedia.org/wiki/Data_Distribution_Service::
  There are several DDS implementations, both commercial and Open
  Source. DDS is a large-scale distribution framework in itself,
  making it an inappropriate choice as an low level distribution
  mechanism for Safir.

RTPS - http://www.omg.org/spec/DDSI/Current::
  The _Real-Time Publish-Subscribe_ (RTPS) protocol can be used for
  one-to-many connectionless communication over transports such as UDP
  Multicast. It is the wire protocol used for DDS interoperability,
  and has its origins in industrial automation. The protocol is
  extensible and quite flexible, and has support for both reliable and
  best-effort communication. It is a candidate for use in the new
  distribution implementation.

ORTE - http://orte.sourceforge.net/::
  An open source implementation of the RTPS protocol. It does not
  appear to be very active, and has slightly unclear licensing
  situation (there is no clear summary of its license on the
  website). This makes it unsuitable for our purposes.

OpenSplice - http://www.prismtech.com/opensplice::
  OpenSplice is an implementation of DDS, and is available under both
  commercial and open source licenses. As stated above we do not want
  or need a complete DDS, but it could have been possible to just use
  OpenSplice's underlying RTPS implementation. Unfortunately it does
  not appear to be easy to separate from the rest of the product, and
  the interface documentation is severly lacking detail. It seems too
  risky to attempt this reuse.

Pragmatic General Multicast - http://en.wikipedia.org/wiki/Pragmatic_General_Multicast::
  PGM is a reliable multicast transport protocol that guarantees an
  ordered sequece of packets without gaps to multiple recipients
  simultaneously.  PGM is a NAK protocol which means that a receiver
  will send a unicast NAK to the sender whenever it detects loss of
  data. Repair data will be sent to recover from data loss if it is
  possible. The protocol also detects if a receiver has ended up in an
  unrecoverable state.

OpenPGM - http://code.google.com/p/openpgm::
  A well-known open source implementation of PGM specification. It
  supports most big platforms such as Windows and Linux and also has a
  beta version for Android.
  
ZeroMQ - http://www.zeromq.org::
  An open source (LGPL) library that offers lightweight message based
  socket-like communication. It offers different kind of services
  where publish-subscribe and peer-to-peer seems to be most
  interesting for us. It handles message fragmentation and always
  delivers complete messages no matter what underlying transport
  protocol being used. ZeroMQ supports TCP and UDP multicast using
  OpenPGM. It also supports inter-thread communication and on Linux it
  even has inter-process communication.  There is no broker or deamon
  that needs to run seperately and the main focus is performance and
  high throughput with minimal locking. It supports many platforms
  including Windows and Linux and has a quite big community.

Bittorrent/P2P::
  This is not a technology that we can use directly per se, but we can
  find useful algorithms and ideas in the enormous amounts of research
  that has been done in academia in this area.

The Spread Toolkit - http://www.spread.org/::
  General message bus, with both singlecast and multicast with and
  without delivery and ordering guarantees. Requires one or several
  server processes to be executing. The open source license includes
  the following GPL incompatible "advertising clause", which prohibits
  us from selecting this toolkit while maintaining the license of
  Safir SDK Core:

  All advertising materials (including web pages) mentioning features or use of 
  this software, or software that uses this software, must display the following 
  acknowledgment: "This product uses software developed by Spread Concepts LLC 
  for use in the Spread toolkit. For more information about Spread see 
  http://www.spread.org"

Advanced Message Queuing Protocol (AMQP) - http://www.amqp.org/about/what::
  AMQP is an open standard for business message passing. The protocol
  relies on a central broker that all messages must pass through.  The
  central boker adds latency and affects the performance
  negatively. Also scalability will suffer from the central broker
  design.

RabbitMQ - http://www.rabbitmq.com::  
  Along with Apache Qpid, RabbitMQ is one of the major implementations of AMQP
  
Apache Qpid - http://qpid.apache.org::
  Along with RabbitMQ, Apache Qpid is one of the major implementations of AMQP.

MassTransit - http://masstransit-project.com/::
  From the web site: ++MassTransit (MT) is a framework for creating
  distributed applications on the .Net platform. MT provides the
  ability to subscribe to messages by type and then connect different
  processing nodes though message subscriptions building a cohesive
  mesh of services.++ Introducing a dependency to a .Net framework
  from the core parts of the Dob, that is implemented in C++, is not
  what we want. Also, although the framework has some support for
  Mono, this is not a natural choice when we have multiplatform
  requirements.

NServiceBus - http://www.nservicebus.com/::
  From the web site: ++Developer-friendly SOA for .Net++.
  This is not useful for Safir, see MassTransit.

[[discarded_solutions]]
==== Discarded Solutions

During the evaluation of these technologies we have come up with several possible designs and ways of using these technologies in our implementation. Here we describe some solutions that were discarded.

===== ZeroMQ

We decided to evaluate *ZeroMQ* since it appeared to be a good choice to build the new communication mechanism on. The licenses, the small footprint as well as the message fragmentation handling in conjunction with the publish-subscribe pattern seemed appealing.

There is quite a bit of functionality that would need to implement on top of ZeroMQ, such as providing a common picture of the system topology, and providing robustness when a node does not keep up with the pace of the rest of the system. ZeroMQ on its own will just discard messages when a peer does not respond. ZeroMQ does not support singlecast over UDP which means that if a singlecast mechanism is needed we have to implement it ourselves or we can decide that it's fine to send addressed data as multicast.

ZeroMQ and the http://zguide.zeromq.org/page:all#Chapter-Advanced-Publish-Subscribe-Patterns[publish-subscribe] pattern would be used for all the low-level communication. Each node will have at least one publisher socket and one subscriber socket, and supported transport protocols are TCP and EPGM which is pgm multicast on top of UDP. The new distribution concept would allow for different node types that specify if TCP or EPGM should be used. It would be allowed to mix nodes using multicast and TCP in the same system.

[[Communication configurations]]
.Communication configurations: To the left a pure multicast system. To the right we have added one TCP node.
image::com_pgm.png["Communication configurations", width=401, link="com_pgm.png"]

ZeroMQ over EPGM is a NAK based protocol, which essentially makes it impossible to know if a sender is transmitting data at a rate which all receivers can't keep up with. To allow us to use this we decided to try to implement some kind of flow control on top of the ZeroMQ EPGM sockets.

A flow controlled socket has a maximum send rate, defined by the node type, that a receiver must obey. If a receiver starts to send NAK's the sender will send RDATA if PGM is used, but if the receiver continues to lose messages, the receiver will eventually get an unrecoverable data loss which will be detected and reported by Communication. This may lead to a pool resynchronization or the node being excluded from the system. An unrecoverable data loss can only be repaired by a complete pool resynchronization, and if a node is constantly demanding pool resynchs it would be excluded from the system.

After writing a few test programs and more evaluation we had to discard ZeroMQ, since it has no reliability guarantees, and no easy way of detecting when data has been lost. This is especially true when using EPGM as the underlying transport. Data can be discarded on the sender side or on the receiver side and it can be lost on the wire. Also, if the sender attempts to send more than ZeroMQ can handle data is discarded silently (see http://lists.zeromq.org/pipermail/zeromq-dev/2011-December/014721.html[] for more information).

We briefly toyed with the idea of improving ZeroMQ to fill in these gaps, but we decided to evaluate "Pure OpenPGM" first. Since ZeroMQ uses OpenPGM for its EPGM transport it was useful to check if we could get OpenPGM to work the way we wanted it to before making any changes to ZeroMQ.  

===== Pure OpenPGM

As mentioned in the previous section we decided to look into using OpenPGM directly. Doing that would allow us to detect lost packages more easily and have a higher degree of control over the transport. After some trials we found a bug in it which caused a lost package, but more seriously than that we realized that controlling the transmission rates would be very difficult or even impossible.

Automatic detection of what transmission rate can be used in a particular configuration is something we judged to be close to impossible to design and implement in a timely fashion. The other alternative would be to use manual rate configuration, and to push this decision back to the user. The problem with this approach is that the configuration is very difficult to get right, and if it is not "exactly right" would make the system either very slow due to the rate being set too low, or very slow due to constant node resynchronizations after overload failures.

Having an underlying communication mechanism that is difficult or even impossible to configure "right" while maintaining performance is a showstopper for the Dob, so any solution using OpenPGM either directly or through ZeroMQ was discarded.

==== Conclusion
After a detailed and thorough evaluation of ZeroMQ/OpenPGM we have realized that there are no existing products that fulfill our requirements without introducing big risks into our project. We have decided to settle on using the *RTPS* protocol or a subset thereof, and do the implementation ourselves.

Note however that we will not be an interoperable DDS or anything like it. We may not even implement the whole of RTPS, instead we will specify a subset that we will implement and that covers our needs.

An advantage of using at least this subset of RTPS is that we have a specification that we can rely on, instead of having to invent and document a protocol ourselves, and there are third party tools (e.g. Wireshark) we can use for debugging.

== Requirements
Requirement on the distribution mechanism and on each sub-component.

=== Communication
* Discover other nodes
. Communication shall automatically discover other nodes in the system.
. All nodes shall have unique id's.
. It shall be possible to inject a seed to the discovery mechanism, that is an address to a node that might exist and be available for contact establishment.
. Communication shall report a node as new only once.
. For a node to be reported as new, Communication must first have received at least one data message from that node.
. When Communication establishes contact with a new node, it shall send all nodes it has received from and all its seeds to the other node.
. A node that has been reported as new can be included in the system.
. A node that has been included in the system can later be excluded.
. An excluded node shall never be allowed to be included in the system again.
* Sending messages
. Communication shall be able to send addressed messages to other included nodes in the system.
. It shall be possible to address a message to all system nodes.
. Communication shall automatically fragment messages to fit the defined MTU.
. Communication shall automatically retransmit messages that have not been aknowledged for a certain amount of time.
. Communication shall automatically send keep-alive messages to all other system nodes if it has been quiet certain amount of time.
. Communication shall use sequence numbers to detect lost messages, duplicated messages, reordered messages etc.
. Communication shall have a limited send buffer. If the host application 
* Receiving messages
. Communication shall deliver messages to the host application in correct order according to sequence numbers without any gaps.
. Communication shall put together fragmented messages before delivering them to the host application.

== Design
This section describes the design of the new distribution mechanism and the related changes to Safir SDK Core.

=== Overview

In the design for the new communication mechanism we have strived to layer the functionality as much as possible. <<block-diagram, Below>> is block diagram that shows the new parts (highlighted) of Safir SDK Core.

[[block-diagram]]
.Block diagram ("new" parts highlighted)
image::block-diagram.png["Block Diagram", width=440, link="block-diagram.png"]

The responsibilities of the new parts can be summarized as follows:

Distribution::
  Consists of two parts:
  * Communication::
        - Provides the low level node to node communication.
        - Discovery of new nodes.
        - Filters received data, to ensure that only data from nodes that are part of the system is handled.
        - Abstracts addressing, so that other parts of the system does not have to worry about IP addresses and port numbers.

  * System Picture::
        - Decides which nodes are allowed to be part of the system.
        - Data routing decisions
        - Sync between nodes.

Control::
  - Starts Safir SDK Core (dose_main, dope_main etc)
  - Starts user applications
  - Decides when/if Dobs on different nodes are allowed to communicate.

Status::
  - Provides Dob objects of the application and group statuses of Control LowLevel.
  - Allows control of applications and groups through the Dob objects.

=== Control - Application Start/Stop

This component will be based on the existing Snac component, with a few notable changes:

 * New name will be "Safir Control" ("Control" will be used in this document, for short), to reflect the fact that this "new" component is somehow in control of the whole system. The user will start safir_control(.exe) which in turn starts dose_main and dope_main and then starts all the other applications. Starting safir_control is a bit less cryptic than starting a lot of xxxx_main applications.
 * Dependencies to ACE will be removed
 * Dependencies to non-Core parts of Safir SDK will be removed
 * Since we want Control to start the Dob it will use the new communication mechanism directly, instead of using the Dob.
 * Will have a low level part and a high level part.
   - The low level part does most of the work, but does 'not' depend on Dose or Dots. This means that low level part will not have the Dob shared memories loaded, and can not be affected by a corrupt shared memory. 
   - The high level part provides Dob objects that behave the same way that the current Snac objects do.
 * Current Master and Slave concepts will most likely be kept.
 * We will create a GUI that can be used to control the system and view its status.
 * We will add a facility which makes it possible for users to inject project-specific information into the Control Dob objects.
   - This makes it possible for a project to collect all system information in one structure.
   - The GUI mentioned above can display project specific information.
A first iteration of Control will probably just launch Safir SDK Core and then launch the existing Snac.

TODO: should we use Boost.Process, not yet part of Boost. http://www.highscore.de/boost/process0.5/

=== System Picture

To be able to provide a system picture ('SP' for short) the System Picture component has to collect communication statistics from Communication. This data consists of timing information and packet loss information, etc. This low level information ('RAW' for short) is collected on all nodes, and distributed to all other nodes. 

From this RAW data System Picture has to produce an SP, that is identical on all nodes. Either this can be done on one node (a 'master' node), or on each node using an algorithm that is guaranteed to produce identical output on all nodes.

An advantage of having the RAW and SP data available on all nodes is that it will be possible to view all of this data on each node (e.g. in Dobexplorer), instead of having to check on each node.

==== Transportation of SP and RAW within a node
Since Control, dose_main and Dobexplorer are separate executables the SP and RAW data has to be transported between them. This will be achieved using shared memory.

So the System Picture in dose_main will publish its RAW data, and the System Picture in Control will publish its RAW and SP data through shared memory.

[[sp-raw-pub-sub]]
.System Picture data
image::sp_raw_pub_sub.png["System Picture Pub/Sub", width=401, link="sp_raw_pub_sub.png"]

==== Exclusion algorithm

We have not decided on an algorithm yet, but it will most likely be based on whether a node is "slowing down" the other nodes to an unacceptable degree.


TODO: Write about the list of node-id:s that can grow very big, and that we want to send cleanup decisions "up" to user or project.

=== Communication
Communication's main responsibilities are to detect new nodes and send and receive messages to/from other nodes. Whenever a new node is detected Communication will report it to the host (System Picture) who decides which nodes are part of the system. Communication shall filter incoming messages and only deliver messages received from other system nodes.
The Communication design is divided into 4 logical parts:

 * _Discoverer_ - Establishes contact with other nodes in the system.
 * _Writer_ - Sends data to other nodes. Retransmits unacknowledged messages and sends keep-alive if there's nothing else to send for a while.
 * _Reader_ - Receives data from sockets. Depending on the characteristics of the received data, internal control messages or application data, it is passed the correct mechanism.
 * _DeliveryHandler_ - The delivery handler is the part of Communication that handles received application data. It orders and defragments incoming data and deliver whole messages to the host application.

[[com-design-overview]]
.Communication overview
image::com_design_overview.png["Communication overview", width=500, link="com_design_overview.png"]

The component is built on mechanisms offered by Boost.Asio, and driven by boost::asio::io_service. The design has been made to have no restrictions of how many threads can run the io_service. See <<com_conc_sync, Concurrency and synchronization>>.

==== Exported interface
An instance of Communication is automatically started when it is constructed and stopped when it is destructed. There are no methods like start and stop or similair.

.Communication interface
[width="80%",frame="topbot",options="header"]
|======================
|Method |Description

|Constructor        
|An instance of Communication is automatically started when it is constructed and stopped when it is destructed. There are no methods like start and stop or similair.

|Destructor
|Disconnects and deletes an instance of Communication.

|Discover
|Takes a collection of seed addresses to other nodes and injects into the _Discoverer_. This instance will then try to contact the seeds locate other nodes.

|IncludeNode
|After a node has been reported as new, it can be included in the system by calling this method. Communication will deliver any data from nodes that are not included in the system.

|ExcludeNode
|A system node can be excluded from the system by calling ExcludeNode. Once a node has been excluded it can never be included again.

|SendAll
|Send data to all system nodes.

|SendTo
|Send data to a specific system node.
|======================

==== Discover nodes
Communication will automatically try to locate other nodes. If seeds have been injected the discover mechanism will cyclically send Discover messages to the seeds until contact has been established. When a node receives a Discover message from another node it will respond with information about itself and all other nodes it has received anything from.
Depending on how many nodes that is, it might have to send more than one message in respond to not exceed the defined MTU.
When a NodeInfo message is received the same procedure is repeated for each node in the NodeInfo that was new to us. I.e we start to send Discover messages to them.
However we do not report any node as new until we have received anything from it, can be a Discover or a NodeInfo in respond to a Discover.

[[com-design-discover]]
.Discover
image::com_design_discover.png["Discover", width=500, link="com_design_discover.png"]
The figure above shows what happens when Node#1 is started and the address to Node#2 is injected as a seed. The figure only shows what will happen in Node#1. It does not show that the other nodes will report Node#1 as new immediately when the Discover is received from Node#1.
If no seeds have been injected, the only thing to do is waiting for other nodes to contact us. As soon as we receive a Discover from another node, we respond with a NodeInfo message and then also send a Discover back to the other node get all its node information. Just the way that Node #2-4 acts in the figure.

==== Send messages
Coming soon

==== Receive messages
Coming soon

[[com_conc_sync]]
==== Concurrency and synchronization
Coming soon

////
==== Protocol
ZeroMQ and the http://zguide.zeromq.org/page:all#Chapter-Advanced-Publish-Subscribe-Patterns[publish-subscribe] pattern will be used for all the low-level communication. Each node will have at least one publisher socket and one subscriber socket, and supported transport protocols are TCP and EPGM which is pgm multicast on top of UDP. The new distribution concept will introduce something called <<NodeTypes, node types>> where it is specified if TCP or EPGM should be used. It will be allowed to mix nodes using multicast and TCP in the same system. What makes this possible is that a multicast node must also be able to talk TCP but a TCP node must not speak multicast.

[[Communication configurations]]
.Communication configurations: To the left a pure multicast system. To the right we have added one TCP node.
image::com_pgm.png["Communication configurations", width=401, link="com_pgm.png"]

To avoid unnecessary copying, message headers and message data will be separated using ZeroMQ's multipart messages. That makes it possible to write the data part in shared memory immediately without any intermediate storage.

[[NodeTypes]]
==== Node types
We introduce a new concept called node type. It shall be possible to define node types with a number of common parameters and then tag every node with a node type. For example one node type could be RT_NODE and another IS_NODE. A node tagged as RT_NODE might have a higher minimum receive data rate than a node tagged as IS_NODE. The node type affects how data are sent to the node and the requirements that must be met to be part of the system.

==== Distribution channels
Distribution channels in the sense of not all data being present at every node will not be supported. We have come to the conclusion that it is not compatible with the dope persistence of today, and it would be very hard (if possible) to make it work. Even todays implementation is deffect.

Broadly the problem is that the system relies on all nodes having all information. If a new node starts and finds that it is not alone, it assumes that it can get a complete pool from the other node, which is not true if there exist persistent data that is not present on the other node due to distribution channels.

==== Flow control
Sockets can be flow controlled (FC) or non flow controlled (NFC).

On a NFC socket the sender is allowed to send as fast as it wants and a receiver is allowed to drop messages if it can't keep up with the pace. Still, if using PGM as transport protocol, NAKs and RData (Repair Data) is sent as usual, so a lost message will only occur in case of PGM unrecoverable data loss. NFC sockets will probably have shorter PGM recovery interval, to reduce memory use, so PGM unrecoverable data losses may not be uncommon when data flow is high.

A FC socket on the other hand has a maximum send rate, defined by the node type, that a receiver must obey. If a receiver starts to send NAK's the sender will sending RDATA if PGM is used, but if the receiver continues to lose messages, the receiver will eventually get an unrecoverable data loss which will be detected and reported by Communication. This may lead to the node being excluded from the system. An unrecoveralbe data loss can only be repaired by a complete pool distribution and probably a node will get a number of chances before exclusion. The exact criteria for node exclusion have not been decided yet.

It may be possible to just have one PGM NFC socket for all node types per distribution channel. PGM recovery interval settings may influence this choice.

==== Priorities
In the current implementation it is possible to set six different priority levels/channels, where each channel can either be acked or unacked. For every type it is then possible to specify which channel it is to be sent on.

The new design removes this level of control. There will not be any priority levels, since we doubt their usefulness (no customer that we know of have actually used the priority levels). Priorities would also compicate things even further.

If in the future we find that priorities are needed, this can be implemented as a weighted round robin algorithm in dose_main. The advantage of this approach is that it would be reasonable easy to implement and it keeps the number of open ports low.

==== Questions
One question that needs more investigation is how big the benefits are of using ZeroMQ's multicast over using OpenPGM directly. ZeroMQ is an additional layer on top of OpenPGM with an interface that looks a bit easier to use. On the downside the simplification means that some features and parameters available in OpenPGM are hidden by ZeroMQ and not easily available to users. ZeroMQ handles message fragmentation which is really nice to have out of the box, however OpenPGM also claims to handle some kind of message fragmentation.
We would also like to use ZeroMQs TCP socket implementation for communication with nodes that are not reachable by UDP multicast.

An issue for us is that there is no ZeroMQ support for unicast UDP. If we can't stick with the existing features of ZeroMQ and have to implement some functionality on our own, lots of the nice stuff in ZeroMQ must be re-implemented anyway.

////

== Dob facilities on top of Communication
The Dob supports three kinds of mechanism: entities, messages and requests (service requests and entity requests). Here we try to show how each of these are realized on the proposed design.

[[uc_messages]]
=== Messages
In the current Dob implementation there are no guarantees that messages are delivered to all subscribers, but if a message is sent on an acked priority level the Dob guarantees that the message will arrive to the node. But after the message has arrived to the node there are no guarantees that it is delivered to all subscribers (if a subscriber's in queue is full the message will not be delivered to that subscriber).

TODO: write about how messages will behave in new implementation

////
The new implementation will always send messages on NFC sockets, often with a smallish recovery interval. To end users this means no logical changes from the current implementation and shall be thought of as exactly the same, but under the hood it will remove the ability to get the weird "delivery-to-node-but-not-to-subscriber-guarantee".

Messages need to be sent on NFC sockets, since if we sent them on FC sockets a node may be excluded due to a lost message.
////

There is a separate article which discusses adding a facility for messages with some kind of guarantees, see the http://safir.sourceforge.net[Safir homepage] for more info.

=== Entities
TODO: write about how entities will behave in new implementation. Will we provide unacked entities?

=== Requests
Requests are always addressed to one receiver and a response is always delivered back to the requestor.

TODO: write about how requests will be implemented.


== Design requirements
 * All nodes must be able to send and receive both unicast and multicast if there is one unicast node
 * A unicast node can only be reached by unicast
 * We do want both unicast and multicast
 * A type is either Acked or not, never both
 * We have to guarantee that important data gets through, regardless of how much other data there is
 * DC and persistence is incompatible (or very difficult)
 * No priorities on socket level (can be added in dose_main by round-robin or something like that)
 * Entities are always acked (unacked entities is an optimization that can be added later)
 * Messages are unacked
 * A "castrato node" must be able to live without all dou-files (object in object, etc). Filtering is done on receiver side.
 * To check: How do dirtysubs queue grow?

