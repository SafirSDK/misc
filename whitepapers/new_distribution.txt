// -*- coding: utf-8 -*-
:encoding: UTF-8

New Distribution Whitepaper
==========================
:Author: Lars Hagström, Anders Widén and Joel Ottosson
:date: 2014-03-31

== Preface
The purpose of this document is to describe the new distribution mechanism of Safir SDK Core and the related changes. It contains information on the benefits, the requirements and the design.

== Benefits
Robust communication::
  Exclusion of misbehaving nodes;;
    New implementation will have an algorithm that will automatically exclude a node that does not behave properly.
  Slow node will not slow down all nodes;;
    Current implementation means that one node that is "slow", either temporarily or always, will slow down distribution to all other nodes.
  Support for other topologies;;
    A number of issues with the current implementation will be addressed:
    - Is really designed and optimized for LAN with TTL=1. It has been made to work with higher TTL. 
    - Has no support for networks where all nodes cannot see all other nodes via UDP multicast.
    - Does not detect situations where firewalls make communication become "one-way".
Removal of general "join" will simplify design::
  The attempt to add general "join" functionality added a lot of complexity that is very unpredictable and hard to explain/document. Removing the general join will simplify the software.

Addition of specific "join" cases will allow more uses::
  Instead of a general join we want to add two kinds of specific join, which will allow nodes to be reconnected if  a network connection is broken. These kinds of nodes will not support having redundant entities:
  Disconnected node will keep all entities owned on other nodes as "read only";;
    An operator console with this setting will still be able to display the "last known" state of all entities. No modification will be allowed and no requests will be served.
  Disconnected node will become empty;;
    An operator console with this setting will still become "empty", i.e. all non-local entities will be deleted (this is the current behavior). When the node is reconnected it will receive all remote entities again.
  TODO;;
    Add description for server node.  

Less static configuration::
  We want to provide a "dhcp"-like functionality for configuring nodes, so that instead of configuring the node identities and other details that should be internal you specify a node type, e.g. "operator console" or "Database server", and from that we internally work out what node id a node should have. This would make adding new nodes a lot easier.
+
*TODO* Is it a problem that a node changes its id? (Peters post-it case)

Easier to display system status::
  We want to provide a unified picture of the system topology, both for use by the system itself to make sure the system remains robust, but this information can also be displayed to allow users and developers to understand what the system status is.

Simpler to start system::
  Less needs for start scripts. All parts of Safir SDK Core and the rest of the system gets started by the same mechanism.

== Product Evaluation
Preferrably some 3rd party products or protocols should be used in the development of the new distribution mechanism, to simplify and speed up the development process, and to reduce maintenance costs.

Some requirements:

Platform independence of 3rd party dependencies::
  Chosen technologies shall run on Win32 and Linux.
License::
  Chosen technologies shall have acceptable dual-licenses so that Safir SDK Core can be licensed under a commercial license as well as GPLv3. See http://www.gnu.org/licenses/license-list.html for the GNU project list of compatible licenses.
Real-Time Behaviour::
  Chosen technologies shall not prevent the Safir SDK Core from having bounded latency.
Reliability::
  Chosen technologies shall not prevent the Safir SDK Core from providing reliability guarantees.
Complexity::
  Chosen technologies shall not add unwarranted complexity to configuration or use.

=== Data format / Serialization
Using an open or standard format for the data packets would remove the need of "rolling our own", and probably also easier to use third party tools such as Wireshark for debugging.

We want a binary format since the blobs are already binary, so XML- or JSON-based formats have not been investigated.

In some distant future we might also want to change the blob format to use an open format, but in the short term we will just wrap them in whatever format we choose.

We only need support for C\+\+, since all the data transport and blob packing/unpacking code is C++ code.

Protobuf - http://code.google.com/p/protobuf/::
  * Google developed data serialization format. 
  * Used by almost all Google internal RPC and file formats. 
  * Well documented and well supported (by Google).
  * There are also multiple implementations, e.g. SAX-like deserialization libraries are available.
  * Stable and widely used.
  * BSD 3-Clause License (GPL compatible)
Apache Thrift - http://thrift.apache.org/::
  * Interfaces to more languages than Protobuf.
  * Slightly larger serialized results compared to Protobuf.
  * Maintenance seems less "professional" than Protobuf.
  * Apache License 2.0 (GPLv3 compatible)

==== Conclusion
*Protobuf* seems to be the sensible choice. Wider usage, better documentation and better maintenance/development.

=== Distribution mechanisms
There are a lot of possible products and protocols available. They all have different characteristics and different levels of guarantees. Here is a list of some of the main ones that we have evaluated along with some of our thoughts on them. See <<discarded_solutions, Discarded Solutions>> for further details into the protocols and products that we have investigated more thoroughly.

DDS - http://en.wikipedia.org/wiki/Data_Distribution_Service::
  There are several DDS implementations, both commercial and Open
  Source. DDS is a large-scale distribution framework in itself,
  making it an inappropriate choice as an low level distribution
  mechanism for Safir.

RTPS - http://www.omg.org/spec/DDSI/Current::
  The _Real-Time Publish-Subscribe_ (RTPS) protocol can be used for
  one-to-many connectionless communication over transports such as UDP
  Multicast. It is the wire protocol used for DDS interoperability,
  and has its origins in industrial automation. The protocol is
  extensible and quite flexible, and has support for both reliable and
  best-effort communication. It is a candidate for use in the new
  distribution implementation.

ORTE - http://orte.sourceforge.net/::
  An open source implementation of the RTPS protocol. It does not
  appear to be very active, and has slightly unclear licensing
  situation (there is no clear summary of its license on the
  website). This makes it unsuitable for our purposes.

OpenSplice - http://www.prismtech.com/opensplice::
  OpenSplice is an implementation of DDS, and is available under both
  commercial and open source licenses. As stated above we do not want
  or need a complete DDS, but it could have been possible to just use
  OpenSplice's underlying RTPS implementation. Unfortunately it does
  not appear to be easy to separate from the rest of the product, and
  the interface documentation is severly lacking detail. It seems too
  risky to attempt this reuse.

Pragmatic General Multicast - http://en.wikipedia.org/wiki/Pragmatic_General_Multicast::
  PGM is a reliable multicast transport protocol that guarantees an
  ordered sequece of packets without gaps to multiple recipients
  simultaneously.  PGM is a NAK protocol which means that a receiver
  will send a unicast NAK to the sender whenever it detects loss of
  data. Repair data will be sent to recover from data loss if it is
  possible. The protocol also detects if a receiver has ended up in an
  unrecoverable state.

OpenPGM - http://code.google.com/p/openpgm::
  A well-known open source implementation of PGM specification. It
  supports most big platforms such as Windows and Linux and also has a
  beta version for Android.
  
ZeroMQ - http://www.zeromq.org::
  An open source (LGPL) library that offers lightweight message based
  socket-like communication. It offers different kind of services
  where publish-subscribe and peer-to-peer seems to be most
  interesting for us. It handles message fragmentation and always
  delivers complete messages no matter what underlying transport
  protocol being used. ZeroMQ supports TCP and UDP multicast using
  OpenPGM. It also supports inter-thread communication and on Linux it
  even has inter-process communication.  There is no broker or deamon
  that needs to run seperately and the main focus is performance and
  high throughput with minimal locking. It supports many platforms
  including Windows and Linux and has a quite big community.

Bittorrent/P2P::
  This is not a technology that we can use directly per se, but we can
  find useful algorithms and ideas in the enormous amounts of research
  that has been done in academia in this area.

The Spread Toolkit - http://www.spread.org/::
  General message bus, with both singlecast and multicast with and
  without delivery and ordering guarantees. Requires one or several
  server processes to be executing. The open source license includes
  the following GPL incompatible "advertising clause", which prohibits
  us from selecting this toolkit while maintaining the license of
  Safir SDK Core:

  All advertising materials (including web pages) mentioning features or use of 
  this software, or software that uses this software, must display the following 
  acknowledgment: "This product uses software developed by Spread Concepts LLC 
  for use in the Spread toolkit. For more information about Spread see 
  http://www.spread.org"

Advanced Message Queuing Protocol (AMQP) - http://www.amqp.org/about/what::
  AMQP is an open standard for business message passing. The protocol
  relies on a central broker that all messages must pass through.  The
  central boker adds latency and affects the performance
  negatively. Also scalability will suffer from the central broker
  design.

RabbitMQ - http://www.rabbitmq.com::  
  Along with Apache Qpid, RabbitMQ is one of the major implementations of AMQP
  
Apache Qpid - http://qpid.apache.org::
  Along with RabbitMQ, Apache Qpid is one of the major implementations of AMQP.

MassTransit - http://masstransit-project.com/::
  From the web site: ++MassTransit (MT) is a framework for creating
  distributed applications on the .Net platform. MT provides the
  ability to subscribe to messages by type and then connect different
  processing nodes though message subscriptions building a cohesive
  mesh of services.++ Introducing a dependency to a .Net framework
  from the core parts of the Dob, that is implemented in C++, is not
  what we want. Also, although the framework has some support for
  Mono, this is not a natural choice when we have multiplatform
  requirements.

NServiceBus - http://www.nservicebus.com/::
  From the web site: ++Developer-friendly SOA for .Net++.
  This is not useful for Safir, see MassTransit.

[[discarded_solutions]]
==== Discarded Solutions

During the evaluation of these technologies we have come up with several possible designs and ways of using these technologies in our implementation. Here we describe some solutions that were discarded.

===== ZeroMQ

We decided to evaluate *ZeroMQ* since it appeared to be a good choice to build the new communication mechanism on. The licenses, the small footprint as well as the message fragmentation handling in conjunction with the publish-subscribe pattern seemed appealing.

There is quite a bit of functionality that would need to implement on top of ZeroMQ, such as providing a common picture of the system topology, and providing robustness when a node does not keep up with the pace of the rest of the system. ZeroMQ on its own will just discard messages when a peer does not respond. ZeroMQ does not support singlecast over UDP which means that if a singlecast mechanism is needed we have to implement it ourselves or we can decide that it's fine to send addressed data as multicast.

ZeroMQ and the http://zguide.zeromq.org/page:all#Chapter-Advanced-Publish-Subscribe-Patterns[publish-subscribe] pattern would be used for all the low-level communication. Each node will have at least one publisher socket and one subscriber socket, and supported transport protocols are TCP and EPGM which is pgm multicast on top of UDP. The new distribution concept would allow for different node types that specify if TCP or EPGM should be used. It would be allowed to mix nodes using multicast and TCP in the same system.

[[Communication configurations]]
.Communication configurations: To the left a pure multicast system. To the right we have added one TCP node.
image::com_pgm.png["Communication configurations", width=401, link="com_pgm.png"]

ZeroMQ over EPGM is a NAK based protocol, which essentially makes it impossible to know if a sender is transmitting data at a rate which all receivers can't keep up with. To allow us to use this we decided to try to implement some kind of flow control on top of the ZeroMQ EPGM sockets.

A flow controlled socket has a maximum send rate, defined by the node type, that a receiver must obey. If a receiver starts to send NAK's the sender will send RDATA if PGM is used, but if the receiver continues to lose messages, the receiver will eventually get an unrecoverable data loss which will be detected and reported by Communication. This may lead to a pool resynchronization or the node being excluded from the system. An unrecoverable data loss can only be repaired by a complete pool resynchronization, and if a node is constantly demanding pool resynchs it would be excluded from the system.

After writing a few test programs and more evaluation we had to discard ZeroMQ, since it has no reliability guarantees, and no easy way of detecting when data has been lost. This is especially true when using EPGM as the underlying transport. Data can be discarded on the sender side or on the receiver side and it can be lost on the wire. Also, if the sender attempts to send more than ZeroMQ can handle data is discarded silently (see http://lists.zeromq.org/pipermail/zeromq-dev/2011-December/014721.html[] for more information).

We briefly toyed with the idea of improving ZeroMQ to fill in these gaps, but we decided to evaluate "Pure OpenPGM" first. Since ZeroMQ uses OpenPGM for its EPGM transport it was useful to check if we could get OpenPGM to work the way we wanted it to before making any changes to ZeroMQ.  

===== Pure OpenPGM

As mentioned in the previous section we decided to look into using OpenPGM directly. Doing that would allow us to detect lost packages more easily and have a higher degree of control over the transport. After some trials we found a bug in it which caused a lost package, but more seriously than that we realized that controlling the transmission rates would be very difficult or even impossible.

Automatic detection of what transmission rate can be used in a particular configuration is something we judged to be close to impossible to design and implement in a timely fashion. The other alternative would be to use manual rate configuration, and to push this decision back to the user. The problem with this approach is that the configuration is very difficult to get right, and if it is not "exactly right" would make the system either very slow due to the rate being set too low, or very slow due to constant node resynchronizations after overload failures.

Having an underlying communication mechanism that is difficult or even impossible to configure "right" while maintaining performance is a showstopper for the Dob, so any solution using OpenPGM either directly or through ZeroMQ was discarded.

==== Conclusion
After a detailed and thorough evaluation of ZeroMQ/OpenPGM we have realized that there are no existing products that fulfill our requirements without introducing big risks into our project. We have decided to settle on using the *RTPS* protocol or a subset thereof, and do the implementation ourselves.

Note however that we will not be an interoperable DDS or anything like it. We may not even implement the whole of RTPS, instead we will specify a subset that we will implement and that covers our needs.

An advantage of using at least this subset of RTPS is that we have a specification that we can rely on, instead of having to invent and document a protocol ourselves, and there are third party tools (e.g. Wireshark) we can use for debugging.

== Beskriv system vs system av system

latens vs bandbredd

iex stödjer hög latens och låg bandbredd

doben stödjer allt från låg till hög latens men förväntar sig hög bandbredd

iex skickar deltan

doben skickar hela state

doben har singelägarskap

iex har delat ägarskap

Att koda mot injektionsdeltan är komplicerat

ett rent dobsystem är lättare att koda för

iex använder man bara när man verkligen behöver det och bara på de typer som behöver skickas över iex-kanalen


== Requirements


=== Introduction

=== Definitions

Node::
        TODO
System::
        TODO

=== Assumptions

* We rely on the IP stack for packet error detection. No additional checksumming on packets will be performed.

* No malicious packets are present on the network. The system will not detect packets that are intentionally trying to disrupt the system.

* Data is sent in the clear. Any encryption shall be handled on the network level, e.g. with an encrypted VPN.

=== Network level requirements

req_1::
    Safir SDK Core shall have no implementation limits to the number of nodes.
    * System characteristics and configuration...
    * Network and CPU capacity and desired data throughput will limit this anyway.
    * Expected size of systems is from one to 500 nodes.

req_2::
    Safir SDK Core shall support both IPv4 and IPv6.
    * One system shall be either IPv4 *or* IPv6, not both.

req_3::
    _Distribution_ shall provide reliable and unreliable transports.
    * This is a requirement between different instances of dose_main, not on app to app level.
    * Implemented with acked and unacked communication.
    * Note that this is a requirement on the network communication mechanism, not a requirement on Safir SDK Core as a whole.

req_4::
    Distributions reliable transport shall guarantee delivery, order and no duplicates.
    * This is a requirement between different instances of dose_main, not on app to app level.
    * It is assumed that the network can be unreliable, produce out of order packets and duplicates.

req_5::
    Distributions unreliable transport shall guarantee order and no duplicates.
    * This is a requirement between different instances of dose_main, not on app to app level.
    * It is assumed that the network can be unreliable, produce out of order packets and duplicates.

req_6::
    Distribution shall be able to use UDP Multicast on networks that support it.
    * No autodetect, configuration.

req_7::
    Distribution shall be able to use UDP Unicast.
    * Useful for nodes that are not reachable by Multicast.

req_8::
    When using UDP Multicast, Distribution shall cope with switches that implement IGPM snooping.
    * Unclear whether there is anything that we need to do, or if this is an OS configuration issue.

req_9::
    TTL shall be configurable in Distribution.

req_10::
    Distribution and Control shall detect and handle situations where the network packets can only be sent one way.
    * e.g. firewall or failed IGMP snooping.
    * Distribution and Control is expected to exclude nodes that do not have two-way communication from the system.

req_11::
    Safir SDK Core shall make it possible to group nodes (into _node types_) according to their characteristics in the system. The characteristics are at least:
    * Multicast or Unicast
    * Dead-timeout
    * Heartbeat interval
    * Sliding window size
    * Ack/retransmission timeout

req_12::
    A slow node in a Safir SDK Core system shall not slow down the rest of the system.
    * A "slow node" is a node that is not able to process incoming data in a timely fashion, either due to a slow network link or due to lacking processing power.
    * "Slowness" does not need to be detected automatically. It is expected to be a configuration issue.

 
req_13::
    Distribution shall automatically discover other nodes in the system.
    * There is no need for a node to have a static list of all other nodes.

req_14::
    It shall be possible to configure a node name that is used for presentation purposes.
    * These shall only be used for presentation purposes, and no checks for duplicates will be made.

req_15::
    It shall be possible to send data payload of arbitrary size.
    * E.g. data packets larger than maximum UDP message size will need to be fragmented.


Note that dose_com, the old distribution mechanism, had _Priority_ and _Distribution Channels_. These mechanisms provided some level of Quality of Service configurability on the network level. The problem with these mechanisms is that they did not affect the system all the way up to the application level. These old QoS mechanisms will not be implemented in the first version of the new distribution mechanism. Adding something similar to these mechanisms is possible in the future, but should take into account all levels, all the way up to the applications.


req_16:: 
    Distribution shall maintain a list of nodes that are part of the system, and keep it synchronized on all nodes.

req_17::
    Control shall use this list (from above) to ensure that only these nodes talk to each other.

req_18::
    Nodes that misbehave shall be excluded from the system by Distribution and Control.

req_19::
    Distribution shall collect statistics that can be used in exclusion algorithms, presentation and debugging.
    * Packet count
    * Latency
    * Packet loss
    * Network hops?


req_20::
    Control shall be able to start dose_main when Distribution says that the current node is part of the system.

req_21::
    Control shall be able to stop dose_main when the current node has been excluded from the system.

req_21::
    Dose shall be able to send data separately to different node type groups.

req_22::
    Control shall start all the other executables in Safir SDK Core, e.g. foreach and dope_main.

req_23::
    Control shall monitor the status of applications that it has started.

req_24::
    Control shall be able to stop applications that it has started.

req_25::
    Status shall be able to mirror the status of Control and Distribution in Dob objects.

req_26::
    Control shall, together with Dose, guarantee that a node that wants to join is in a correct state to join.

req_27::
    Distribution shall use a CRC32 checksum to ensure data integrity when transmitting over the network.

=== Quality of Service

==== Distribution channels
In the current implementation there is a feature known as a _Distribution Channel_. Distribution Channels make it possible to send certain types of data to only a subset of nodes. This is an optimization, which can reduce CPU load if used wisely, since it allows data filtering on the network card (through multicast groups).

Distribution channels will not be supported in this general sense in the new implementation. We have come to the conclusion that it is not compatible with the dope persistence of today, and it would be very hard (if possible at all) to make it work. Even the current implementation is defective.

In short the problem is that the system relies on all nodes having all information. If a new node starts and finds that it is not alone, it assumes that it can get a complete pool from the other node, which is not true if there exist persistent data that is not present on the other node due to distribution channels.

==== Priorities
In the current implementation it is possible to set six different priority levels/channels, where each channel can either be acked or unacked. For every type it is then possible to specify which channel it is to be sent on.

The new design removes this level of control. There will not be any priority levels, since we doubt their usefulness (no customer that we know of have actually used the priority levels). Priorities would also complicate things even further.

If, in the future, we find that priorities are needed, this can be implemented as a weighted round robin algorithm in dose_main. The advantage of this approach is that it would be reasonable easy to implement and it keeps the number of open ports low.

Also, the priorities of today do not propagate all the way up into the application, but are only on a network level. To implement effective priorities there should be prioritization all the way up into the applications.

=== TODOs

Seed förväntas vara till bara ett system, om man seedar till flera system så får man skylla sig själv.

Ska inkarnationsnumret på något sätt ingå i alla paket? Så att vi kan garantera att alla paket tillhör inkarnationen.

TODO: Lägg till inkarnationsbegreppet!

Skriv Dope-krav och ta bort fulredundansvarianten. Lås i db!

Objektsdefinitioner ska vara lika på alla noder. Parametrar får vara olika. Kanske nån annan mekanism.

Requirement on the distribution mechanism and on each sub-component.

Little/big endian?

Ta med prioriteter i krav och design, men implementera inte. På vilken nivå ska prioriteterna ligga?

== Sequences etc.

=== Node startup sequences

A node in its very basic form consists of two executables; Control and dose_main. System Picture and Communication are each instantiated in both of these executables.

The System Picture in the Control executable (known as the master) is the one that is able to produce a system picture, and the one in the dose_main executable (known as the slave) just produces statistics and listens to his master.

[[node-diagram]]
.Deployed components in a node
image::node.png["Node Diagram", width=445, link="node.png"]

.Some Definitions
[width="50%",cols="15%,85%",frame="topbot",options="header"]
|======================
| | Explanation

| SP~m~ 
| System Picture Master, i.e. the one that runs in Control

| [SP~m~]
| Coordinating System Picture Master, i.e. the one that runs in Control and has been elected Coordinator

| SP~s~
| System Picture Slave, i.e. the one that runs in dose_main

| Curly braces
| Candidate to join the System.

| Square braces
| Elected coordinator

|======================


==== One already running (B), another starting (A)

.Communication interface
[width="80%",frame="topbot",options="header"]
|======================
|Node A | Node B

|COM~m~: Discover
|COM~m~: NodeInfo

|SP~m~ -> COM~m~: Include B
|SP~m~ -> COM~m~: Include A

|SP~m~: Xmit RAW
|SP~m~: Xmit RAW & SP (101, [B], \{A})

|SP~m~ -> Ctrl: SP (101, [B], \{A})
|[SP~m~] -> Ctrl: SP (101, [B], \{A})

|SP~m~ -> SP~s~: SP (101, [B], \{A})
|[SP~m~] -> SP~s~: SP (101, [B], \{A})

|SP~s~ -> dose: SP (101, [B], \{A})
|SP~s~ -> dose: SP (101, [B], \{A})

|Ctrl -> SP~m~: Join 101
|

|SP~m~: Xmit Join 101
|

|
|[SP~m~]: Xmit SP (101, [B], A)

|SP~m~ -> Ctrl: SP (101, [B], A)
|[SP~m~] -> Ctrl: SP (101, [B], A)

|SP~m~ -> SP~s~: SP (101, [B], A)
|[SP~m~] -> SP~s~: SP (101, [B], A)

|SP~s~ -> dose: SP (101, [B], A)
|SP~s~ -> dose: SP (101, [B], A)

|dose -> COM~s~: connect to B
|dose -> COM~s~: connect to A

|======================


*Lemma*: When SP~m~ has new SP it will immediately be sent to Ctrl and SP~s~. As soon as SP~s~ gets a new SP it will be sent to dose_main. I.e. As soon as SP~m~ has a new SP everyone will get it. From now on this will be written as "SP~m~: Updated SP(...)".

==== One starting, alone


[width="40%",frame="topbot",options="header"]
|======================
|Node B

| COM~m~: Sends discover if he has any seeds

| SP~m~: No NewNode in X seconds -> Start election -> Becomes coordinator

| [SP~m~]: Updated SP(null,[B])

| Ctrl -> [SP~m~]: New incarnation number (101)

| [SP~m~]: Updated SP(101,[B])

|======================

==== Two starting at the same time, no others

.Communication interface
[width="80%",frame="topbot",options="header"]
|======================
|Node A | Node B

|COM~m~: Discover
|COM~m~: NodeInfo

|SP~m~ -> COM~m~: Include B
|SP~m~ -> COM~m~: Include A

|SP~m~: Xmit RAW
|SP~m~: Xmit RAW

| SP~m~: No coordinator in X seconds, start election (?)
| SP~m~: No coordinator in X seconds, start election

| 
| B becomes coordinator

|
| [SP~m~]: Updated SP (null, [B], \{A})

|
| Ctrl -> [SP~m~]: New incarnation number (102)

|
| [SP~m~]: Updated SP (102, [B], \{A})

|======================

Now we're in the case with one node joining an existing system, see above.

=== Communication
* Discover other nodes
. Communication shall automatically discover other nodes in the system.
. All nodes shall have unique id's.
. It shall be possible to inject a seed to the discovery mechanism, that is an address to a node that might exist and be available for contact establishment.
. Communication shall report a node as new only once.
. For a node to be reported as new, Communication must first have received at least one data message from that node.
. When Communication establishes contact with a new node, it shall send all nodes it has received from and all its seeds to the other node.
. A node that has been reported as new can be included in the system.
. A node that has been included in the system can later be excluded.
. An excluded node shall never be allowed to be included in the system again.
* Sending messages
. Communication shall be able to send addressed messages to other included nodes in the system.
. It shall be possible to address a message to all system nodes.
. Communication shall automatically fragment messages to fit the defined MTU.
. Communication shall automatically retransmit messages that have not been aknowledged for a certain amount of time.
. Communication shall automatically send keep-alive messages to all other system nodes if it has been quiet certain amount of time.
. Communication shall use sequence numbers to detect lost messages, duplicated messages, reordered messages etc.
. Communication shall have a limited send buffer. If the host application 
* Receiving messages
. Communication shall deliver messages to the host application in correct order according to sequence numbers without any gaps.
. Communication shall put together fragmented messages before delivering them to the host application.

== Design
This section describes the design of the new distribution mechanism and the related changes to Safir SDK Core.

=== Overview

In the design for the new communication mechanism we have strived to layer the functionality as much as possible. <<block-diagram, Below>> is block diagram that shows the new parts (highlighted) of Safir SDK Core.

[[block-diagram]]
.Block diagram ("new" parts highlighted)
image::block-diagram.png["Block Diagram", width=440, link="block-diagram.png"]

The responsibilities of the new parts can be summarized as follows:

Distribution::
  Consists of two parts:
  * Communication::
        - Provides the low level node to node communication.
        - Discovery of new nodes.
        - Filters received data, to ensure that only data from nodes that are part of the system is handled.
        - Abstracts addressing, so that other parts of the system does not have to worry about IP addresses and port numbers.

  * System Picture::
        - Decides which nodes are allowed to be part of the system.
        - Data routing decisions
        - Sync between nodes.

Control::
  - Starts Safir SDK Core (dose_main, dope_main etc)
  - Starts user applications
  - Decides when/if Dobs on different nodes are allowed to communicate.
  - Owns all configuration

Status::
  - Provides Dob objects of the application and group statuses of Control LowLevel.
  - Allows control of applications and groups through the Dob objects.

=== Node types

A system can have several node types, and one node is of one and only one node type. Each node type has a number of characteristics that control how communication with nodes that belong to it will happen.

.Some Definitions
[width="70%",cols="20%,15%,65%",frame="topbot",options="header"]
|======================
| Property Name | Type | Comment

| Name 
| string 
| Name of the node type

| IsLight 
| boolean
| Is th enode a _light_ node (see below).

| TalksTo
| list of string
| List of node types that nodes of this type can talk to. \'\*' if all node types. Only for nodes where IsCastrato is true can this be anything other than \'*'.

| Multicast
| boolean
| Is the node able to communicate via multicast

| DeadTimeout
| duration
| How much time may pass without getting a packet from this node before marking it as dead.

| HeartbeatInterval
| duration
| How often shall heartbeats/keepalives be sent.

| SlidingWindowSize
| int
| Size of the sliding window when communicating with this node.

| RetryTimeout
| duration
| Time to wait for Ack before retrying transmission to this node.

| Wanted types
| list of regex
| All types that match will be sent to the node (unless they also match an Unwanted, below). Only for nodes where IsLight is true can this be anything other than \'.*'.

| Unwanted types
| list of regex
| Types that match will not be sent to the node. Must be empty if IsLight is false.

|======================

All nodes have the full node type configuration available at startup, and when a node starts it knows of which node type it is.

[[light_nodes]]
==== Light nodes

One of the most fundamental node type properties is whether a node is a _light_ node. Light nodes are the _only_ kind of nodes that, after it has been disconnected (e.g. network cable has been disconnected), can rejoin a system without being restarted. A light node is for example only allowed to own any global entities that are unique, which means that when it is rejoined into the system there can be no conflicts between entity owners.

The properties of a light node are:

* Can only register handlers for services that are marked as local or _limited_ (see below).
* Can only register handlers for entities that are marked as local or limited.
* Can (obviously) only own instances of entity types that are marked as local or limited.
* Can subscribe to everything
* Will only receive messages, requests and entity instances from nodes that can TalkTo it.
* Can send messages


When a light node has been disconnected from a system and then reconnected to the same or a different system it will start a new Discovery. It may also change its IP address while it is disconnected, since the discovery will ensure that the new IP address is known by its counterparts.

A light node may connect to a system with a different incarnation number after disconnection. I.e. it should not remember incarnation numbers when it is disconnected.

===== Limited types

A limited type is an Entity or Service type with a few limitations that make it possible for light nodes to register handlers and own instances of them. The limitations are there to ensure that a light node can reconnect to a system after it has been disconnected.

* A limited type can have no persistence, i.e. it has neither Volatile, Permanent or Injectable persistence.
* Pending registrations of a limited type is not allowed.
* Overregistration of a limited type is considered a programming or configuration error.
 - An error will be logged if an overregistration is detected.
 - There is no guarantee that all overregistrations will be detected.

===== Registrations and ownership of limited types

All regular (i.e. non-light) nodes forward changes in registrations on limited types to everyone. This will ensure that everyone gets changes in registration on limited types, although it will produce a certain amount of overhead. The overhead is only registration states of limited types, so it should be only a small amount of traffic in all sensible systems.

The forwarding of registrations will ensure that two light nodes cannot have the same type registered, and hence it will ensure that ownership is consistent. Of course it does not guarantee that all nodes contain the same information, since some entity instances may not be present on all nodes. But that will be obvious to the user because of the TalksTo field in the node type.

===== Light nodes and incomplete dou-set

A light can contain only a subset (it must be a consistent subset, of course) of the dou files. 

If dose_main receives data of a type that it does not know about it will discard it, and for objects that are embedded in other objects the data will be unpacked to the base type, which dots knows about.

=== Control - Application Start/Stop

This component will be based on the existing Snac component, with a few notable changes:

 * New name will be "Safir Control" ("Control" will be used in this document, for short), to reflect the fact that this "new" component is somehow in control of the whole system. The user will start safir_control(.exe) which in turn starts dose_main and dope_main and then starts all the other applications. Starting safir_control is a bit less cryptic than starting a lot of xxxx_main applications.
 * Dependencies to ACE will be removed
 * Dependencies to non-Core parts of Safir SDK will be removed
 * Since we want Control to start the Dob it will use the new communication mechanism directly, instead of using the Dob.
 * Will have a low level part and a high level part.
   - The low level part does most of the work, but does 'not' depend on Dose or Dots. This means that low level part will not have the Dob shared memories loaded, and can not be affected by a corrupt shared memory. 
   - The high level part provides Dob objects that behave the same way that the current Snac objects do.
 * Current Master and Slave concepts will most likely be kept.
 * We will create a GUI that can be used to control the system and view its status.
 * We will add a facility which makes it possible for users to inject project-specific information into the Control Dob objects.
   - This makes it possible for a project to collect all system information in one structure.
   - The GUI mentioned above can display project specific information.
A first iteration of Control will probably just launch Safir SDK Core and then launch the existing Snac.

TODO: should we use Boost.Process, not yet part of Boost. http://www.highscore.de/boost/process0.5/

=== System Picture

To be able to provide a system picture ('SP' for short) the System Picture component has to collect communication statistics from Communication. This data consists of timing information and packet loss information, etc. This low level information ('RAW' for short) is collected on all nodes, and distributed to all other nodes. 

From this RAW data System Picture has to produce an SP, that is identical on all nodes. This will be done by having one SP act as _Coordinator_, which is responsible for producing and distributing the SP.

An advantage of having the RAW and SP data available on all nodes is that it will be possible to view all of this data on each node (e.g. in Dobexplorer), instead of having to check on each node.

==== Electing a Coordinator

System Picture uses the Bully Algorithm to elect a coordinator.

==== Transportation of SP and RAW within a node
Since Control, dose_main and Dobexplorer are separate executables the SP and RAW data has to be transported between them. This will be achieved using IPC (domain sockets on Linux and named pipes on Windows).

So the System Picture in dose_main will publish its RAW data, and the System Picture in Control will publish its RAW and SP data through IPC.

[[sp-raw-pub-sub]]
.System Picture data
image::sp_raw_pub_sub.png["System Picture Pub/Sub", width=401, link="sp_raw_pub_sub.png"]

==== Exclusion algorithm

We have not decided on an algorithm yet, but it will most likely be based on whether a node is "slowing down" the other nodes to an unacceptable degree.


TODO: Write about the list of node-id:s that can grow very big, and that we want to send cleanup decisions "up" to user or project.

=== Communication
Communication's main responsibilities are to detect new nodes and send and receive messages to/from other nodes. Whenever a new node is detected Communication will report it to the host (System Picture) who decides which nodes are part of the system. Communication shall filter incoming messages and only deliver messages received from other system nodes.
The Communication design is divided into 4 logical parts:

 * _Discoverer_ - Establishes contact with other nodes in the system.
 * _Writer_ - Sends data to other nodes. Retransmits unacknowledged messages and sends keep-alive if there's nothing else to send for a while.
 * _Reader_ - Receives data from sockets. Depending on the characteristics of the received data, internal control messages or application data, it is passed the correct mechanism.
 * _DeliveryHandler_ - The delivery handler is the part of Communication that handles received application data. It orders and defragments incoming data and deliver whole messages to the host application.

[[com-design-overview]]
.Communication overview
image::com_design_overview.png["Communication overview", width=500, link="com_design_overview.png"]

The component is built on mechanisms offered by Boost.Asio, and driven by boost::asio::io_service. The design has been made to have no restrictions of how many threads can run the io_service. See <<com_conc_sync, Concurrency and synchronization>>.

==== Exported interface
An instance of Communication is automatically started when it is constructed and stopped when it is destructed. There are no methods like start and stop or similair.

.Communication interface
[width="80%",frame="topbot",options="header"]
|======================
|Method |Description

|Constructor        
|An instance of Communication is automatically started when it is constructed and stopped when it is destructed. There are no methods like start and stop or similair.

|Destructor
|Disconnects and deletes an instance of Communication.

|Discover
|Takes a collection of seed addresses to other nodes and injects into the _Discoverer_. This instance will then try to contact the seeds locate other nodes.

|IncludeNode
|After a node has been reported as new, it can be included in the system by calling this method. Communication will deliver any data from nodes that are not included in the system.

|ExcludeNode
|A system node can be excluded from the system by calling ExcludeNode. Once a node has been excluded it can never be included again.

|SendAll
|Send data to all system nodes.

|SendTo
|Send data to a specific system node.
|======================

==== Discover nodes
Communication will automatically try to locate other nodes. If seeds have been injected the discover mechanism will cyclically send Discover messages to the seeds until contact has been established. When a node receives a Discover message from another node it will respond with information about itself and all other nodes it has received anything from.
Depending on how many nodes that is, it might have to send more than one message in respond to not exceed the defined MTU.
When a NodeInfo message is received the same procedure is repeated for each node in the NodeInfo that was new to us. I.e we start to send Discover messages to them.
However we do not report any node as new until we have received anything from it, can be a Discover or a NodeInfo in respond to a Discover.

[[com-design-discover]]
.Discover
image::com_design_discover.png["Discover", width=500, link="com_design_discover.png"]
The figure above shows what happens when Node#1 is started and the address to Node#2 is injected as a seed. The figure only shows what will happen in Node#1. It does not show that the other nodes will report Node#1 as new immediately when the Discover is received from Node#1.
If no seeds have been injected, the only thing to do is waiting for other nodes to contact us. As soon as we receive a Discover from another node, we respond with a NodeInfo message and then also send a Discover back to the other node get all its node information. Just the way that Node #2-4 acts in the figure.

==== Send messages
Coming soon

==== Receive messages
Coming soon

[[com_conc_sync]]
==== Concurrency and synchronization
Coming soon


==== Questions

== Dob facilities on top of Communication
The Dob supports three kinds of mechanism: entities, messages and requests (service requests and entity requests). Here we try to show how each of these are realized on the proposed design.

[[uc_messages]]
=== Messages
In the current Dob implementation there are no guarantees that messages are delivered to all subscribers, but if a message is sent on an acked priority level the Dob guarantees that the message will arrive to the node. But after the message has arrived to the node there are no guarantees that it is delivered to all subscribers (if a subscriber's in queue is full the message will not be delivered to that subscriber). 
This "delivery-to-node-but-not-to-subscriber-guarantee" is hard to explain and we have not been able to figure out any relevant use case where it could be used. Contrary to this, the new implementation will have two distinct, and hopefully, easy-to-explain concepts; messages that are guaranteed to be delivered to the receiving applications and messages that don't have this guarantee.

==== Messages without guaranteed delivery
Messages without guaranteed delivery are typically used for cyclic data where a few lost messages are acceptable. For example, messages can be lost due to network failures or if some application cannot keep up with the message rate. There is no way for the sender to know whether or not a message actually has been delivered.

Messages of this type are always sent unacked on the network level.

[[delivered_messages]]
==== Messages with guaranteed delivery
Messages with guaranteed delivery are used when messages must be delivered to all current subscribers. The sender is informed about the outcome of the send operation, i.e. if the message was successfully delivered to all subscribers or if the delivery to one or more subscribers did fail.

The current approach is to use the Foreach mechanism to implement messages with guaranteed delivery. There is a separate article which discusses the details, see the http://safir.sourceforge.net[Safir homepage] for more info. The implementation is based on that service requests are used and that the message subscribers do register service handlers for global service types.

=== Entities
TODO: write about how entities will behave in new implementation. Will we provide unacked entities?

=== Requests
Requests are always addressed to one receiver and a response is always delivered back to the requestor.

TODO: write about how requests will be implemented.


== Design requirements
 * All nodes must be able to send and receive both unicast and multicast if there is one unicast node
 * A unicast node can only be reached by unicast
 * We do want both unicast and multicast
 * A type is either Acked or not, never both
 * We have to guarantee that important data gets through, regardless of how much other data there is
 * DC and persistence is incompatible (or very difficult)
 * No priorities on socket level (can be added in dose_main by round-robin or something like that)
 * Entities are always acked (unacked entities is an optimization that can be added later)

== Miscellaneous issues

=== How do the subscription queues grow in dose_internal?

The worry was whether a slow node may cause other nodes to start using a large amount of memory to handle its subscription queues. I.e. we want to double check that they do not grow infinitely.

This is not a problem, an already dirty subscription will not be added again to the subscription queue.

